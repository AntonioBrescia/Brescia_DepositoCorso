{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372c0310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count: 13\n",
      "\n",
      "Training base models...\n",
      "  Model XGBRegressor OOF RMSE: 0.07509082241032099\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2869\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 1.056618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2869\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 1.057530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2868\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 1.057269\n",
      "  Model LGBMRegressor OOF RMSE: 0.07545734065541558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 17:24:56,588] A new study created in memory with name: no-name-c2942b34-30ea-4dee-a692-6e9006babca8\n",
      "[I 2025-12-09 17:24:56,594] Trial 0 finished with value: 0.07300901097451133 and parameters: {'w0': 0.824111534178575, 'w1': 0.618659728281595, 'w2': 0.8095639779271999}. Best is trial 0 with value: 0.07300901097451133.\n",
      "[I 2025-12-09 17:24:56,597] Trial 1 finished with value: 0.07332519101656457 and parameters: {'w0': 0.8674805044309565, 'w1': 0.6124279250159466, 'w2': 0.5401562194178464}. Best is trial 0 with value: 0.07300901097451133.\n",
      "[I 2025-12-09 17:24:56,599] Trial 2 finished with value: 0.07294660725813408 and parameters: {'w0': 0.36878701740290065, 'w1': 0.2898798409279797, 'w2': 0.4021737551495408}. Best is trial 2 with value: 0.07294660725813408.\n",
      "[I 2025-12-09 17:24:56,601] Trial 3 finished with value: 0.07295126305656814 and parameters: {'w0': 0.1386305272745546, 'w1': 0.9235439121196705, 'w2': 0.7946375490288725}. Best is trial 2 with value: 0.07294660725813408.\n",
      "[I 2025-12-09 17:24:56,604] Trial 4 finished with value: 0.07395427224301154 and parameters: {'w0': 0.19985011018033916, 'w1': 0.8690443588007877, 'w2': 0.22763032786739712}. Best is trial 2 with value: 0.07294660725813408.\n",
      "[I 2025-12-09 17:24:56,606] Trial 5 finished with value: 0.07334527440936035 and parameters: {'w0': 0.48848360614467456, 'w1': 0.6122160359319672, 'w2': 0.39624564294560716}. Best is trial 2 with value: 0.07294660725813408.\n",
      "[I 2025-12-09 17:24:56,610] Trial 6 finished with value: 0.07308292219498863 and parameters: {'w0': 0.2389828401458609, 'w1': 0.20357496595607605, 'w2': 0.22367783157745824}. Best is trial 2 with value: 0.07294660725813408.\n",
      "[I 2025-12-09 17:24:56,613] Trial 7 finished with value: 0.07286826858054904 and parameters: {'w0': 0.8304050461995243, 'w1': 0.34891796512672124, 'w2': 0.8479425157362779}. Best is trial 7 with value: 0.07286826858054904.\n",
      "[I 2025-12-09 17:24:56,616] Trial 8 finished with value: 0.07321474565681202 and parameters: {'w0': 0.9931143370107385, 'w1': 0.8421170879674152, 'w2': 0.7753107326564437}. Best is trial 7 with value: 0.07286826858054904.\n",
      "[I 2025-12-09 17:24:56,620] Trial 9 finished with value: 0.0731935281491779 and parameters: {'w0': 0.7056409857469523, 'w1': 0.8557943405858816, 'w2': 0.6876605317956447}. Best is trial 7 with value: 0.07286826858054904.\n",
      "[I 2025-12-09 17:24:56,632] Trial 10 finished with value: 0.0725526098015701 and parameters: {'w0': 0.6336452637559618, 'w1': 0.03099717571843763, 'w2': 0.9496971229668998}. Best is trial 10 with value: 0.0725526098015701.\n",
      "[I 2025-12-09 17:24:56,639] Trial 11 finished with value: 0.07253738100148674 and parameters: {'w0': 0.6257951136429908, 'w1': 0.06467767679633854, 'w2': 0.9715755931454948}. Best is trial 11 with value: 0.07253738100148674.\n",
      "[I 2025-12-09 17:24:56,648] Trial 12 finished with value: 0.07248003794492142 and parameters: {'w0': 0.5836441579945015, 'w1': 0.002675334447304341, 'w2': 0.9991512699954878}. Best is trial 12 with value: 0.07248003794492142.\n",
      "[I 2025-12-09 17:24:56,654] Trial 13 finished with value: 0.07247801229206588 and parameters: {'w0': 0.5718644543469186, 'w1': 0.02235408380888282, 'w2': 0.9855762106840397}. Best is trial 13 with value: 0.07247801229206588.\n",
      "[I 2025-12-09 17:24:56,663] Trial 14 finished with value: 0.07425735983462074 and parameters: {'w0': 0.4415202265573039, 'w1': 0.09801587821046198, 'w2': 0.05070082361177458}. Best is trial 13 with value: 0.07247801229206588.\n",
      "[I 2025-12-09 17:24:56,670] Trial 15 finished with value: 0.07250867485327635 and parameters: {'w0': 0.01931845596987103, 'w1': 0.41922115525367726, 'w2': 0.6385557775492633}. Best is trial 13 with value: 0.07247801229206588.\n",
      "[I 2025-12-09 17:24:56,678] Trial 16 finished with value: 0.07234505076566529 and parameters: {'w0': 0.34995205512079475, 'w1': 0.2000142926397455, 'w2': 0.9866679208011407}. Best is trial 16 with value: 0.07234505076566529.\n",
      "[I 2025-12-09 17:24:56,685] Trial 17 finished with value: 0.07239925437381417 and parameters: {'w0': 0.3783675083852756, 'w1': 0.1848658995319839, 'w2': 0.8959072121590217}. Best is trial 16 with value: 0.07234505076566529.\n",
      "[I 2025-12-09 17:24:56,692] Trial 18 finished with value: 0.0725388902463194 and parameters: {'w0': 0.33528260068548527, 'w1': 0.20544704026206606, 'w2': 0.6308265991636794}. Best is trial 16 with value: 0.07234505076566529.\n",
      "[I 2025-12-09 17:24:56,700] Trial 19 finished with value: 0.07236465526680137 and parameters: {'w0': 0.3153394442400276, 'w1': 0.18778207127994673, 'w2': 0.8539627513438843}. Best is trial 16 with value: 0.07234505076566529.\n",
      "[I 2025-12-09 17:24:56,707] Trial 20 finished with value: 0.07250465274360567 and parameters: {'w0': 0.02242970521334775, 'w1': 0.4639290965978231, 'w2': 0.7118385142511556}. Best is trial 16 with value: 0.07234505076566529.\n",
      "[I 2025-12-09 17:24:56,717] Trial 21 finished with value: 0.07234808917476958 and parameters: {'w0': 0.31848893614517076, 'w1': 0.17609021622187096, 'w2': 0.8830254679558059}. Best is trial 16 with value: 0.07234505076566529.\n",
      "[I 2025-12-09 17:24:56,725] Trial 22 finished with value: 0.07239373891660053 and parameters: {'w0': 0.2676231971131843, 'w1': 0.2901937202477012, 'w2': 0.8533844078711597}. Best is trial 16 with value: 0.07234505076566529.\n",
      "[I 2025-12-09 17:24:56,733] Trial 23 finished with value: 0.07217495633678431 and parameters: {'w0': 0.12587641292714172, 'w1': 0.1419870841683255, 'w2': 0.884643476879813}. Best is trial 23 with value: 0.07217495633678431.\n",
      "[I 2025-12-09 17:24:56,740] Trial 24 finished with value: 0.07216513568015427 and parameters: {'w0': 0.10588840903293936, 'w1': 0.10347420802019743, 'w2': 0.7490538055125762}. Best is trial 24 with value: 0.07216513568015427.\n",
      "[I 2025-12-09 17:24:56,750] Trial 25 finished with value: 0.07251783807552502 and parameters: {'w0': 0.12318504690182616, 'w1': 0.3090651752973487, 'w2': 0.5355985863953767}. Best is trial 24 with value: 0.07216513568015427.\n",
      "[I 2025-12-09 17:24:56,756] Trial 26 finished with value: 0.07219142681283118 and parameters: {'w0': 0.13266895528318381, 'w1': 0.11089517235017454, 'w2': 0.7474755338599893}. Best is trial 24 with value: 0.07216513568015427.\n",
      "[I 2025-12-09 17:24:56,765] Trial 27 finished with value: 0.07217930257328495 and parameters: {'w0': 0.10908670425791264, 'w1': 0.11664501607776609, 'w2': 0.7281794608281604}. Best is trial 24 with value: 0.07216513568015427.\n",
      "[I 2025-12-09 17:24:56,771] Trial 28 finished with value: 0.07218911614307164 and parameters: {'w0': 0.08081196698406527, 'w1': 0.12081799346693482, 'w2': 0.6050232511849628}. Best is trial 24 with value: 0.07216513568015427.\n",
      "[I 2025-12-09 17:24:56,781] Trial 29 finished with value: 0.07266846222818481 and parameters: {'w0': 0.18607416683487843, 'w1': 0.5211736972765202, 'w2': 0.694889908577585}. Best is trial 24 with value: 0.07216513568015427.\n",
      "[I 2025-12-09 17:24:56,788] Trial 30 finished with value: 0.07337289484405883 and parameters: {'w0': 0.005571271510188408, 'w1': 0.7322264390949242, 'w2': 0.4048094338246673}. Best is trial 24 with value: 0.07216513568015427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model CatBoostRegressor OOF RMSE: 0.07212925033367072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 17:24:56,798] Trial 31 finished with value: 0.07218302766489323 and parameters: {'w0': 0.08114951447012972, 'w1': 0.11619279142057216, 'w2': 0.6132526867921302}. Best is trial 24 with value: 0.07216513568015427.\n",
      "[I 2025-12-09 17:24:56,807] Trial 32 finished with value: 0.07226469049904458 and parameters: {'w0': 0.07650688162936439, 'w1': 0.1381449690354789, 'w2': 0.47027092114941793}. Best is trial 24 with value: 0.07216513568015427.\n",
      "[I 2025-12-09 17:24:56,817] Trial 33 finished with value: 0.07253317341690954 and parameters: {'w0': 0.2395685840912828, 'w1': 0.26402691081973795, 'w2': 0.5800276586143724}. Best is trial 24 with value: 0.07216513568015427.\n",
      "[I 2025-12-09 17:24:56,823] Trial 34 finished with value: 0.07213177732764255 and parameters: {'w0': 0.08071851293088785, 'w1': 0.07546931318458584, 'w2': 0.787899445030191}. Best is trial 34 with value: 0.07213177732764255.\n",
      "[I 2025-12-09 17:24:56,835] Trial 35 finished with value: 0.07218571918658392 and parameters: {'w0': 0.16228232510862117, 'w1': 0.0571252585640199, 'w2': 0.7888097409386357}. Best is trial 34 with value: 0.07213177732764255.\n",
      "[I 2025-12-09 17:24:56,846] Trial 36 finished with value: 0.07227702022892717 and parameters: {'w0': 0.07124849213248777, 'w1': 0.34652108543395477, 'w2': 0.9301613365453982}. Best is trial 34 with value: 0.07213177732764255.\n",
      "[I 2025-12-09 17:24:56,854] Trial 37 finished with value: 0.07235001327796467 and parameters: {'w0': 0.1984819617085589, 'w1': 0.23598914462517906, 'w2': 0.7386877311315793}. Best is trial 34 with value: 0.07213177732764255.\n",
      "[I 2025-12-09 17:24:56,862] Trial 38 finished with value: 0.07215491805124097 and parameters: {'w0': 0.12515738503480828, 'w1': 0.06950794146131652, 'w2': 0.8248878291031785}. Best is trial 34 with value: 0.07213177732764255.\n",
      "[I 2025-12-09 17:24:56,869] Trial 39 finished with value: 0.07227201651695267 and parameters: {'w0': 0.2578652474604445, 'w1': 0.06877326644739216, 'w2': 0.8068003583301472}. Best is trial 34 with value: 0.07213177732764255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best weights: [0.085499   0.07993892 0.83456209]\n",
      "\n",
      "========================\n",
      " FINAL TEST RMSE: 0.25188199526878957\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ===========================================\n",
    "# 1) LOAD DATA\n",
    "# ===========================================\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target.copy()\n",
    "X.columns = X.columns.str.lower()\n",
    "\n",
    "# ===========================================\n",
    "# 2) WINSORIZING (IQR)\n",
    "# ===========================================\n",
    "def winsorize_iqr(df, k=1.5):\n",
    "    w = df.copy()\n",
    "    for col in df.columns:\n",
    "        Q1 = w[col].quantile(0.25)\n",
    "        Q3 = w[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - k * IQR\n",
    "        upper = Q3 + k * IQR\n",
    "        w[col] = np.clip(w[col], lower, upper)\n",
    "    return w\n",
    "\n",
    "X = winsorize_iqr(X)\n",
    "\n",
    "# ===========================================\n",
    "# 3) FEATURE ENGINEERING\n",
    "# ===========================================\n",
    "X[\"bedrooms_per_room\"] = X[\"avebedrms\"] / (X[\"averooms\"] + 1e-6)\n",
    "X[\"population_density\"] = X[\"population\"] / (X[\"aveoccup\"] + 1e-6)\n",
    "X[\"log_medinc\"] = np.log1p(X[\"medinc\"])\n",
    "\n",
    "# KMeans GEO clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=RANDOM_STATE)\n",
    "X[\"geo_cluster\"] = kmeans.fit_predict(X[[\"latitude\", \"longitude\"]])\n",
    "\n",
    "# KNN local price\n",
    "knn = KNeighborsRegressor(n_neighbors=15, weights=\"distance\")\n",
    "knn.fit(X[[\"latitude\", \"longitude\"]], y)\n",
    "X[\"knn_price\"] = knn.predict(X[[\"latitude\", \"longitude\"]])\n",
    "\n",
    "print(\"Feature count:\", X.shape[1])\n",
    "\n",
    "# TARGET LOG TRANSFORM\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# TRAIN/TEST SPLIT + SCALING\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, y_log, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ===========================================\n",
    "# 4) BASE MODELS\n",
    "# ===========================================\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=1500, learning_rate=0.03,\n",
    "    max_depth=6, subsample=0.9, colsample_bytree=0.8,\n",
    "    tree_method=\"hist\", random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=1500, learning_rate=0.03,\n",
    "    num_leaves=80, subsample=0.9, colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=1500, learning_rate=0.03,\n",
    "    depth=6, random_seed=RANDOM_STATE, verbose=False\n",
    ")\n",
    "\n",
    "models = [xgb_model, lgb_model, cat_model]\n",
    "\n",
    "# ===========================================\n",
    "# 5) K-FOLD OOF\n",
    "# ===========================================\n",
    "def get_oof(model):\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof = np.zeros_like(y_train)\n",
    "    test_preds = np.zeros((3, len(y_test)))\n",
    "\n",
    "    for i, (tr, val) in enumerate(kf.split(X_train)):\n",
    "        m = model\n",
    "        m.fit(X_train[tr], y_train[tr])\n",
    "        oof[val] = m.predict(X_train[val])\n",
    "        test_preds[i] = m.predict(X_test)\n",
    "\n",
    "    return oof, test_preds.mean(axis=0)\n",
    "\n",
    "oofs = []\n",
    "tests = []\n",
    "\n",
    "print(\"\\nTraining base models...\")\n",
    "for m in models:\n",
    "    o, t = get_oof(m)\n",
    "    oofs.append(o)\n",
    "    tests.append(t)\n",
    "    print(f\"  Model {m.__class__.__name__} OOF RMSE:\", np.sqrt(mean_squared_error(y_train, o)))\n",
    "\n",
    "# ===========================================\n",
    "# 6) OPTUNA ENSEMBLE WEIGHTS\n",
    "# ===========================================\n",
    "def ensemble_rmse(weights):\n",
    "    w = np.array(weights)\n",
    "    w = w / w.sum()\n",
    "    pred = np.zeros_like(oofs[0])\n",
    "    for i in range(len(oofs)):\n",
    "        pred += w[i] * oofs[i]\n",
    "    return np.sqrt(mean_squared_error(y_train, pred))\n",
    "\n",
    "def objective(trial):\n",
    "    ws = [trial.suggest_float(f\"w{i}\", 0, 1) for i in range(len(models))]\n",
    "    return ensemble_rmse(ws)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=40, show_progress_bar=False)\n",
    "\n",
    "best_w = np.array([study.best_params[f\"w{i}\"] for i in range(len(models))])\n",
    "best_w = best_w / best_w.sum()\n",
    "print(\"\\nBest weights:\", best_w)\n",
    "\n",
    "# ===========================================\n",
    "# 7) FINAL TEST PREDICTION\n",
    "# ===========================================\n",
    "ens_test = np.zeros_like(tests[0])\n",
    "for w, t in zip(best_w, tests):\n",
    "    ens_test += w * t\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(ens_test)))\n",
    "print(\"\\n========================\")\n",
    "print(\" FINAL TEST RMSE:\", rmse)\n",
    "print(\"========================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
