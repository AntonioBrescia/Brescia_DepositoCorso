{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4501b144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count: 13\n",
      "\n",
      "Training Stacking Model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# ===========================================\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# 6) TRAIN STACKING MODEL\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# ===========================================\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining Stacking Model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43mstacking\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# ===========================================\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# 7) TEST SET PERFORMANCE\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# ===========================================\u001b[39;00m\n\u001b[32m    120\u001b[39m preds = stacking.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:1043\u001b[39m, in \u001b[36mStackingRegressor.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m   1039\u001b[39m _raise_for_params(fit_params, \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, allow=[\u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1041\u001b[39m y = column_or_1d(y, warn=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:211\u001b[39m, in \u001b[36m_BaseStacking.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    206\u001b[39m             \u001b[38;5;28mself\u001b[39m.estimators_.append(estimator)\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28mself\u001b[39m.estimators_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdrop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28mself\u001b[39m.named_estimators_ = Bunch()\n\u001b[32m    220\u001b[39m est_fitted_idx = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ===========================================\n",
    "# 1) LOAD DATA\n",
    "# ===========================================\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target.copy()\n",
    "X.columns = X.columns.str.lower()\n",
    "\n",
    "# ===========================================\n",
    "# 2) WINSORIZING IQR\n",
    "# ===========================================\n",
    "def winsorize_iqr(df, k=1.5):\n",
    "    w = df.copy()\n",
    "    for col in df.columns:\n",
    "        Q1 = w[col].quantile(0.25)\n",
    "        Q3 = w[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - k * IQR\n",
    "        upper = Q3 + k * IQR\n",
    "        w[col] = np.clip(w[col], lower, upper)\n",
    "    return w\n",
    "\n",
    "X = winsorize_iqr(X)\n",
    "\n",
    "# ===========================================\n",
    "# 3) FEATURE ENGINEERING\n",
    "# ===========================================\n",
    "X[\"bedrooms_per_room\"] = X[\"avebedrms\"] / (X[\"averooms\"] + 1e-6)\n",
    "X[\"population_density\"] = X[\"population\"] / (X[\"aveoccup\"] + 1e-6)\n",
    "X[\"log_medinc\"] = np.log1p(X[\"medinc\"])\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=RANDOM_STATE)\n",
    "X[\"geo_cluster\"] = kmeans.fit_predict(X[[\"latitude\", \"longitude\"]])\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=15, weights=\"distance\")\n",
    "knn.fit(X[[\"latitude\", \"longitude\"]], y)\n",
    "X[\"knn_price\"] = knn.predict(X[[\"latitude\", \"longitude\"]])\n",
    "\n",
    "print(\"Feature count:\", X.shape[1])\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# TRAIN/TEST SPLIT + SCALING\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, y_log, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ===========================================\n",
    "# 4) BASE MODELS (Level 0)\n",
    "# ===========================================\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=1000, learning_rate=0.03,\n",
    "    max_depth=6, subsample=0.9, colsample_bytree=0.8,\n",
    "    tree_method=\"hist\", random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=1000, learning_rate=0.03,\n",
    "    num_leaves=60, subsample=0.9, colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=1000, learning_rate=0.03,\n",
    "    depth=6, random_seed=RANDOM_STATE, verbose=False\n",
    ")\n",
    "\n",
    "estimators = [\n",
    "    (\"xgb\", xgb_model),\n",
    "    (\"lgb\", lgb_model),\n",
    "    (\"cat\", cat_model)\n",
    "]\n",
    "\n",
    "# ===========================================\n",
    "# 5) META-LEARNER (Level 1)\n",
    "# ===========================================\n",
    "final_estimator = RidgeCV()\n",
    "\n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=final_estimator,\n",
    "    cv=5,\n",
    "    passthrough=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ===========================================\n",
    "# 6) TRAIN STACKING MODEL\n",
    "# ===========================================\n",
    "print(\"\\nTraining Stacking Model...\")\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "# ===========================================\n",
    "# 7) TEST SET PERFORMANCE\n",
    "# ===========================================\n",
    "preds = stacking.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(preds)))\n",
    "\n",
    "print(\"\\n========================\")\n",
    "print(\" STACKING TEST RMSE:\", rmse)\n",
    "print(\"========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de18fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e Pulizia Dati...\n",
      "   -> Dataset ridotto al 10%: (1965, 9)\n",
      "   -> Dataset pulito: (1829, 9)\n",
      "2. Generazione Feature 'Combo'...\n",
      "   -> Totale Feature Generate: 105\n",
      "3. Selezione Feature tramite GPU...\n",
      "   -> Feature Sopravvissute: 41\n",
      "\n",
      "4. Tuning Iperparametri con Optuna e Early Stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-12-10 14:07:32,882] Trial 0 failed with parameters: {'learning_rate': 0.0334011444805488, 'depth': 8, 'l2_leaf_reg': 4.796379848754684, 'bagging_temperature': 3.653156347181304} because of the following error: CatBoostError('catboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 35: CUDA driver version is insufficient for CUDA runtime version').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\39327\\AppData\\Local\\Temp\\ipykernel_10312\\4102767642.py\", line 175, in objective_cat\n",
      "    model.fit(X_opt_train, y_opt_train, eval_set=[(X_opt_val, y_opt_val)], early_stopping_rounds=100)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                     use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                     verbose_eval, metric_period, silent, early_stopping_rounds,\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                     save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "    ~~~~~~~~~~~^\n",
      "        train_pool,\n",
      "        ^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        train_params[\"init_model\"]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 35: CUDA driver version is insufficient for CUDA runtime version\n",
      "[W 2025-12-10 14:07:32,885] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 35: CUDA driver version is insufficient for CUDA runtime version",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCatBoostError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(mean_squared_error(y_opt_val, preds))\n\u001b[32m    179\u001b[39m study_cat = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[43mstudy_cat\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# --- LightGBM ---\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobjective_lgbm\u001b[39m(trial):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 175\u001b[39m, in \u001b[36mobjective_cat\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    159\u001b[39m params = {\n\u001b[32m    160\u001b[39m     \u001b[33m'\u001b[39m\u001b[33miterations\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m4000\u001b[39m,\n\u001b[32m    161\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.005\u001b[39m, \u001b[32m0.1\u001b[39m, log=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrandom_state\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m42\u001b[39m\n\u001b[32m    173\u001b[39m }\n\u001b[32m    174\u001b[39m model = CatBoostRegressor(**params)\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_opt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_opt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_opt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_opt_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m preds = model.predict(X_opt_val)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(mean_squared_error(y_opt_val, preds))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\catboost\\core.py:5873\u001b[39m, in \u001b[36mCatBoostRegressor.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5872\u001b[39m     CatBoostRegressor._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5874\u001b[39m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5875\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5876\u001b[39m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\catboost\\core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\39327\\OneDrive\\Documenti\\GitHub\\Brescia_DepositoCorso\\.venv\\Lib\\site-packages\\catboost\\core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCatBoostError\u001b[39m: catboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 35: CUDA driver version is insufficient for CUDA runtime version"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import warnings\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset e Preprocessing\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Modelli\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Configurazione\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. CARICAMENTO E PULIZIA DATI\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Caricamento e Pulizia Dati...\")\n",
    "start_global = time.time()\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Rimuoviamo il cap a 5.0\n",
    "df = df[df['MedHouseVal'] < 5.0]\n",
    "\n",
    "# Riduzione dataset al 10%\n",
    "df = df.sample(frac=0.10, random_state=42).reset_index(drop=True)\n",
    "print(f\"   -> Dataset ridotto al 10%: {df.shape}\")\n",
    "\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    indices_to_drop = []\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df_clean[(df_clean[col] < Q1 - 2.0*IQR) | (df_clean[col] > Q3 + 2.0*IQR)].index\n",
    "        indices_to_drop.extend(outliers)\n",
    "    return df_clean.drop(list(set(indices_to_drop)))\n",
    "\n",
    "cols_clean = ['AveRooms', 'AveBedrms', 'AveOccup', 'MedInc']\n",
    "df = remove_outliers_iqr(df, cols_clean)\n",
    "print(f\"   -> Dataset pulito: {df.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Generazione Feature 'Combo'...\")\n",
    "\n",
    "def generate_comprehensive_features(df_input, cols_to_combine):\n",
    "    df_eng = df_input.copy()\n",
    "    math_cols = [c for c in cols_to_combine if c not in ['Latitude', 'Longitude', 'Geo_Cluster']]\n",
    "    \n",
    "    # Rotazioni coordinate\n",
    "    df_eng['Rot_45_LatLon'] = df_eng['Latitude'] + df_eng['Longitude']\n",
    "    df_eng['Rot_N45_LatLon'] = df_eng['Latitude'] - df_eng['Longitude']\n",
    "\n",
    "    # Logaritmi\n",
    "    for col in math_cols:\n",
    "        if df_eng[col].min() >= 0:\n",
    "            df_eng[f'LOG_{col}'] = np.log1p(df_eng[col])\n",
    "\n",
    "    # Moltiplicazioni\n",
    "    for col1, col2 in itertools.combinations(math_cols, 2):\n",
    "        df_eng[f'MULT_{col1}_x_{col2}'] = df_eng[col1] * df_eng[col2]\n",
    "\n",
    "    # Divisioni\n",
    "    for col1, col2 in itertools.permutations(math_cols, 2):\n",
    "        df_eng[f'RATIO_{col1}_div_{col2}'] = df_eng[col1] / (df_eng[col2] + 1e-5)\n",
    "\n",
    "    return df_eng\n",
    "\n",
    "# Feature geografiche\n",
    "sf_coords = (37.7749, -122.4194)\n",
    "la_coords = (34.0522, -118.2437)\n",
    "df['Dist_SF'] = np.sqrt((df['Latitude'] - sf_coords[0])**2 + (df['Longitude'] - sf_coords[1])**2)\n",
    "df['Dist_LA'] = np.sqrt((df['Latitude'] - la_coords[0])**2 + (df['Longitude'] - la_coords[1])**2)\n",
    "\n",
    "coords = df[['Latitude', 'Longitude']]\n",
    "kmeans = KMeans(n_clusters=15, random_state=42, n_init=10)\n",
    "df['Geo_Cluster'] = kmeans.fit_predict(StandardScaler().fit_transform(coords))\n",
    "\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "cols_for_math = [c for c in X.columns if c != 'Geo_Cluster']\n",
    "X_full = generate_comprehensive_features(X, cols_for_math)\n",
    "X_full.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_full.fillna(0, inplace=True)\n",
    "print(f\"   -> Totale Feature Generate: {X_full.shape[1]}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. SELEZIONE FEATURE\n",
    "# ---------------------------------------------------------\n",
    "print(\"3. Selezione Feature tramite CPU...\")\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "selector_model = XGBRegressor(\n",
    "    n_estimators=500, max_depth=8, learning_rate=0.05,\n",
    "    tree_method='hist', n_jobs=1, random_state=42\n",
    ")\n",
    "selector_model.fit(X_full, y)\n",
    "selection = SelectFromModel(selector_model, prefit=True, threshold='1.25*median')\n",
    "X_selected = X_full.loc[:, selection.get_support()]\n",
    "print(f\"   -> Feature Sopravvissute: {X_selected.shape[1]}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "N_TRIALS = 20\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. OPTUNA TUNING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# --- XGBoost ---\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': 4000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "        'tree_method': 'hist',\n",
    "        'n_jobs': 1,\n",
    "        'random_state': 42,\n",
    "        'early_stopping_rounds': 100\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train, eval_set=[(X_opt_val, y_opt_val)], verbose=False)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS)\n",
    "\n",
    "# --- CatBoost ---\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': 4000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 6, 12),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'task_type': 'CPU',          # CPU mode\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'RMSE'\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train, eval_set=[(X_opt_val, y_opt_val)], early_stopping_rounds=100)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=N_TRIALS)\n",
    "\n",
    "# --- LightGBM ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': 4000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 200),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.95),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10),\n",
    "        'device': 'cpu',             # CPU mode\n",
    "        'n_jobs': 1,\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'metric': 'rmse'\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    callbacks = [early_stopping(100, verbose=False), log_evaluation(period=0)]\n",
    "    model.fit(X_opt_train, y_opt_train, eval_set=[(X_opt_val, y_opt_val)], eval_metric='rmse', callbacks=callbacks)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=N_TRIALS)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. COSTRUZIONE Mixture of Experts (MoE)\n",
    "# ---------------------------------------------------------\n",
    "best_xgb_params = study_xgb.best_params\n",
    "best_xgb_params.update({'n_estimators': 2000, 'tree_method': 'hist', 'n_jobs': 1, 'random_state': 42})\n",
    "if 'early_stopping_rounds' in best_xgb_params: del best_xgb_params['early_stopping_rounds']\n",
    "\n",
    "best_cat_params = study_cat.best_params\n",
    "best_cat_params.update({'iterations': 2000, 'task_type': 'CPU', 'verbose': 0, 'random_state': 42})\n",
    "\n",
    "best_lgbm_params = study_lgbm.best_params\n",
    "best_lgbm_params.update({'n_estimators': 2000, 'device': 'cpu', 'n_jobs': 1, 'verbosity': -1, 'random_state': 42})\n",
    "\n",
    "class SmartSoftModelSelector(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators, selector_model=None):\n",
    "        self.estimators = estimators\n",
    "        self.selector_model = selector_model if selector_model else XGBClassifier(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "            tree_method='hist', n_jobs=1, random_state=42\n",
    "        )\n",
    "        self.model_names = [name for name, _ in estimators]\n",
    "    def fit(self, X, y):\n",
    "        self.fitted_estimators_ = []\n",
    "        for name, model in self.estimators:\n",
    "            model.fit(X, y)\n",
    "            self.fitted_estimators_.append(model)\n",
    "        errors = pd.DataFrame()\n",
    "        for name, model in self.estimators:\n",
    "            oof_preds = cross_val_predict(model, X, y, cv=5, n_jobs=1)\n",
    "            errors[name] = np.abs(y - oof_preds)\n",
    "        y_best_model_idx = errors.idxmin(axis=1).apply(lambda x: self.model_names.index(x))\n",
    "        self.selector_model.fit(X, y_best_model_idx)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        base_preds = np.column_stack([model.predict(X) for model in self.fitted_estimators_])\n",
    "        weights = self.selector_model.predict_proba(X)\n",
    "        final_pred = np.sum(base_preds * weights, axis=1)\n",
    "        return final_pred\n",
    "\n",
    "estimators_list = [\n",
    "    ('xgb', XGBRegressor(**best_xgb_params)),\n",
    "    ('cat', CatBoostRegressor(**best_cat_params)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm_params)) \n",
    "]\n",
    "moe_model = SmartSoftModelSelector(estimators=estimators_list)\n",
    "moe_model.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. CORREZIONE RESIDUI\n",
    "# ---------------------------------------------------------\n",
    "class ResidualCorrectedMoE(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, base_moe_model, corrector_model=None):\n",
    "        self.base_moe_model = base_moe_model\n",
    "        self.corrector_model = corrector_model if corrector_model else CatBoostRegressor(\n",
    "            iterations=500, depth=6, learning_rate=0.03, l2_leaf_reg=5,\n",
    "            task_type='CPU', verbose=0, allow_writing_files=False, random_state=42\n",
    "        )\n",
    "    def fit(self, X, y):\n",
    "        oof_preds = cross_val_predict(self.base_moe_model, X, y, cv=5, n_jobs=1)\n",
    "        residuals = y - oof_preds\n",
    "        self.corrector_model.fit(X, residuals)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        base_pred = self.base_moe_model.predict(X)\n",
    "        correction = self.corrector_model.predict(X)\n",
    "        return base_pred + correction\n",
    "\n",
    "final_system = ResidualCorrectedMoE(base_moe_model=moe_model)\n",
    "final_system.fit(X_train, y_train)\n",
    "\n",
    "y_pred_corrected = final_system.predict(X_test)\n",
    "final_rmse_corr = np.sqrt(mean_squared_error(y_test, y_pred_corrected))\n",
    "final_r2_corr = r2_score(y_test, y_pred_corrected)\n",
    "\n",
    "print(f\"\\n==========================================\")\n",
    "print(f\" RISULTATI FINALI OTTIMIZZATI\")\n",
    "print(f\"==========================================\")\n",
    "print(f\" RMSE: {final_rmse_corr:.5f}\")\n",
    "print(f\" R^2 : {final_r2_corr:.5f}\")\n",
    "print(f\"==========================================\")\n",
    "\n",
    "# Plot veloce\n",
    "corrections_test = final_system.corrector_model.predict(X_test)\n",
    "base_preds_test = final_system.base_moe_model.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=base_preds_test, y=corrections_test, alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title(\"Bias Correction vs Price (Optimized Models)\")\n",
    "plt.xlabel(\"Predicted Price\")\n",
    "plt.ylabel(\"Correction\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
